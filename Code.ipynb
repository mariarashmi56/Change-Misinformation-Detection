{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preapre training data\n",
    "\n",
    "import json\n",
    "\n",
    "train_path = 'C:\\\\Users\\\\maria\\\\Desktop\\\\nlp\\\\Assignment 2\\\\project-files\\\\project-files\\\\train.json'\n",
    "dev_path = 'C:\\\\Users\\\\maria\\\\Desktop\\\\nlp\\\\Assignment 2\\\\project-files\\\\project-files\\\\dev.json'\n",
    "neg_path = 'C:\\\\Users\\\\maria\\\\Desktop\\\\nlp\\\\Assignment 2\\\\project-files\\\\project-files\\\\neg-train-output.json'\n",
    "\n",
    "encode_path = ('train.json','dev.json')\n",
    "\n",
    "\n",
    "def get_content_from_json(file_path):\n",
    "    document_list = []\n",
    "    label_list = []\n",
    "    with open(file_path) as json_file:\n",
    "        if file_path.endswith(encode_path):\n",
    "            data = json.load(json_file)   \n",
    "            for item in data:\n",
    "                text = data[item]['text'].encode('ascii','ignore').decode(\"utf-8\")  #Encode Decode \\u\n",
    "                document_list.append(text) \n",
    "                label_list.append(data[item]['label'])\n",
    "        else:  \n",
    "            data = json.load(json_file)   \n",
    "            for item in data:\n",
    "                text = data[item]['text']\n",
    "                document_list.append(text) \n",
    "                label_list.append(data[item]['label'])\n",
    "            \n",
    "    return document_list, label_list\n",
    "\n",
    "pos_data,pos_label= get_content_from_json(train_path)\n",
    "neg_data,neg_label = get_content_from_json(neg_path)\n",
    "dev_data,dev_label = get_content_from_json(dev_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pos_data+neg_data\n",
    "train_label = pos_label+neg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1410"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prepare test data\n",
    "import json\n",
    "\n",
    "\n",
    "test_path = 'C:\\\\Users\\\\maria\\\\Desktop\\\\nlp\\\\Assignment 2\\\\project-files\\\\project-files\\\\test-unlabelled.json'\n",
    "\n",
    "\n",
    "def get_content_from_json(file_path):\n",
    "    document_list = []\n",
    "    with open(file_path) as json_file:\n",
    "        data = json.load(json_file)   \n",
    "        for item in data:\n",
    "            text = data[item]['text'].encode('ascii','ignore').decode(\"utf-8\")  #Encode Decode \\u\n",
    "            document_list.append(text)                \n",
    "    return document_list\n",
    "\n",
    "\n",
    "test_data = get_content_from_json(test_path)\n",
    "\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2804, 100, 1410)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "stopwords = set(stopwords.words('English'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def document_preprocessing(data):\n",
    "    \n",
    "    token_clean_corpus= []\n",
    "    #tokenize\n",
    "    token_corpus =  [word_tokenize(doc) for doc in data]\n",
    "\n",
    "    for doc in token_corpus:\n",
    "        #remove punctuation\n",
    "        strip_token = [word.translate(table) for word in doc]\n",
    "        #handle casing\n",
    "        lower_token = [word.lower() for word in strip_token]\n",
    "        #remove numbers \n",
    "        only_words = [word for word in lower_token if word.isalpha()]\n",
    "        #remove stopwords\n",
    "        token_no_stopword = [word for word in only_words if word not in stopwords]\n",
    "        #stem words\n",
    "        stem_token = [porter_stemmer.stem(word) for word in token_no_stopword]\n",
    "        token_clean_corpus.append(stem_token)\n",
    "        \n",
    "    return token_clean_corpus\n",
    "    \n",
    "train_preprocessed_data = document_preprocessing(train_data)\n",
    "dev_preprocessed_data = document_preprocessing(dev_data) \n",
    "test_preprocessed_data = document_preprocessing(test_data) \n",
    "len(train_preprocessed_data),len(dev_preprocessed_data),len(test_preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def join_text(preprocess_data):\n",
    "    join_data = [' '.join(i) for i in preprocess_data]\n",
    "    return join_data\n",
    "\n",
    "\n",
    "train_sent =  join_text(train_preprocessed_data)\n",
    "dev_sent = join_text(dev_preprocessed_data)\n",
    "test_sent = join_text(test_preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "train_set = tf_idf.fit_transform(train_sent)\n",
    "develop_set = tf_idf.transform(dev_sent)\n",
    "test_set = tf_idf.transform(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.898\n",
      "[[46  4]\n",
      " [ 6 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90        50\n",
      "           1       0.92      0.88      0.90        50\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score,confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "svc = SVC(C=10)\n",
    "svc.fit(train_set,train_label)\n",
    "pred=svc.predict(develop_set)\n",
    "score = f1_score(dev_label, pred)\n",
    "print('F1 Score: %.3f' % score)\n",
    "cm = confusion_matrix(dev_label,pred)\n",
    "print(cm)\n",
    "print(classification_report(dev_label,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train model on train and dev data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posdev_data = [dev_data[idx] for idx,val in enumerate(dev_label) if val == 1]\n",
    "posdev_label = [1] * len(posdev_data)\n",
    "negdev_data = [dev_data[idx] for idx,val in enumerate(dev_label) if val == 0]\n",
    "negdev_label = [0] * len(negdev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = pos_data+posdev_data\n",
    "pos_label = pos_label+posdev_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data= neg_data+negdev_data\n",
    "neg_label = neg_label+negdev_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pos_data + neg_data\n",
    "train_label = pos_label + neg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed_data = document_preprocessing(train_data)\n",
    "dev_preprocessed_data = document_preprocessing(dev_data) \n",
    "test_preprocessed_data = document_preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent =  join_text(train_preprocessed_data)\n",
    "dev_sent = join_text(dev_preprocessed_data)\n",
    "test_sent = join_text(test_preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "train_set = tf_idf.fit_transform(train_sent)\n",
    "develop_set = tf_idf.transform(dev_sent)\n",
    "test_set = tf_idf.transform(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 1.000\n",
      "[[50  0]\n",
      " [ 0 50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      1.00      1.00        50\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score,confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "svc = SVC(C=10)\n",
    "svc.fit(train_set,train_label)\n",
    "pred=svc.predict(develop_set)\n",
    "score = f1_score(dev_label, pred)\n",
    "print('F1 Score: %.3f' % score)\n",
    "cm = confusion_matrix(dev_label,pred)\n",
    "print(cm)\n",
    "print(classification_report(dev_label,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predict test and convert to json file\n",
    "\n",
    "test_pred = svc.predict(test_set)\n",
    "\n",
    "pred_dict = {}\n",
    "\n",
    "for i, val in enumerate(test_pred):\n",
    "    pred_dict[\"test-\"+str(i)] = {\"label\": int(val)}\n",
    "\n",
    "with open('C:\\\\Users\\\\maria\\\\Desktop\\\\nlp\\\\Assignment 2\\\\project-files\\\\project-files\\\\test-output.json', 'w') as outfile:\n",
    "    json.dump(pred_dict, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
